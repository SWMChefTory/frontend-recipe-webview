import { useEffect, useRef, useState } from 'react';

// TEN VAD exports (ë„ˆê°€ ì˜¬ë¦° index ê¸°ì¤€)
import type { TenVADInstance } from 'ten-vad-lib';
import { VADInstance, VADModuleLoader } from 'ten-vad-lib';

// ONNX Runtime Web for KWS
import * as ort from 'onnxruntime-web';

const STT_URL = 'wss://api.cheftories.com/api/v1/voice-command/ws';
const SAMPLE_RATE = 16000;
const CHUNK_SIZE = 160; // 10ms @ 16kHz
const BUFFER_CHUNKS = 3; // 30ms ë¬¶ìŒ
const SEND_SIZE = CHUNK_SIZE * BUFFER_CHUNKS;

// VAD ê²Œì´íŒ… íˆìŠ¤í…Œë¦¬ì‹œìŠ¤
const POS_TH = 0.6; // ì¼œì§ ì„ê³„
const NEG_TH = 0.5; // êº¼ì§ ì„ê³„
const ON_HOLD_MS = 150; // ì¼œì§ ìœ ì§€(ë°”ìš´ìŠ¤ ë°©ì§€)
const OFF_HOLD_MS = 250; // êº¼ì§ ì§€ì—°(í„´ ì¢…ë£Œ ì•ˆì •)

// Pre-buffer ì„¤ì • (ìŒì„± ì•ë¶€ë¶„ ë³´í˜¸)
const PRE_BUFFER_MS = 200; // 200ms í”„ë¦¬ë²„í¼
const PRE_BUFFER_CHUNKS = Math.ceil((PRE_BUFFER_MS * SAMPLE_RATE) / 1000 / CHUNK_SIZE); // ~12.5 ì²­í¬

const f32ToI16 = (f: Float32Array) => {
  const i = new Int16Array(f.length);
  for (let n = 0; n < f.length; n++) {
    const v = Math.max(-1, Math.min(1, f[n]));
    i[n] = (v * 0x7fff) | 0;
  }
  return i;
};

// ì„œë²„ í”„ë¡œí† ì½œì— ë§ê²Œ is_final í”Œë˜ê·¸ì™€ í•¨ê»˜ ì˜¤ë””ì˜¤ ì „ì†¡
const sendAudioData = (ws: WebSocket, audioData: ArrayBuffer, isFinal: boolean = false) => {
  const finalFlag = new Uint8Array([isFinal ? 1 : 0]);
  const audioBytes = new Uint8Array(audioData);
  const combined = new Uint8Array(finalFlag.length + audioBytes.length);
  combined.set(finalFlag, 0);
  combined.set(audioBytes, finalFlag.length);
  ws.send(combined.buffer);
};

interface Params {
  selectedSttModel?: string;
  accessToken?: string | null;
  recipeId?: string;
  onVoiceStart?: () => void;
  onVoiceEnd?: () => void;
  onIntent?: (i: any) => void; // BasicIntent ì“°ë©´ íƒ€ì… êµì²´
  onVolume?: (vol: number) => void;
  onKwsDetection?: (probability: number) => void;
  onKwsActivate?: () => void;
  onKwsDeactivate?: () => void;
}

export const useSimpleSpeech = ({
  selectedSttModel = 'CLOVA',
  accessToken,
  recipeId,
  onVoiceStart,
  onVoiceEnd,
  onIntent,
  onVolume,
  onKwsDetection,
  onKwsActivate,
  onKwsDeactivate,
}: Params) => {
  const [error, setError] = useState<string | null>(null);

  // WS
  const wsRef = useRef<WebSocket | null>(null);
  const isWSReady = useRef(false);
  const reconnectTimeoutRef = useRef<number | null>(null);

  // ì˜¤ë””ì˜¤
  const audioCtxRef = useRef<AudioContext | null>(null);
  const processorRef = useRef<AudioWorkletNode | null>(null);
  const streamRef = useRef<MediaStream | null>(null);

  // TEN VAD
  const vadInstanceRef = useRef<TenVADInstance | null>(null);

  // KWS (Keyword Spotting)
  const kwsSessionRef = useRef<ort.InferenceSession | null>(null);
  const kwsBufferRef = useRef<Float32Array>(new Float32Array(0));
  const kwsEmaRef = useRef<number | null>(null);
  const kwsSustainMsRef = useRef<number>(0);
  const kwsArmedRef = useRef<boolean>(false);
  const kwsActivatedRef = useRef<boolean>(false);
  const kwsTimeoutRef = useRef<NodeJS.Timeout | null>(null);

  // ìƒíƒœ
  const isMountedRef = useRef(true);
  const txLeftoverRef = useRef<Float32Array | null>(null);
  const preBufferRef = useRef<Float32Array[]>([]); // Pre-buffer for speech start protection

  // VAD ê²Œì´íŒ… ìƒíƒœ
  const speechActiveRef = useRef(false);
  const lastOnRef = useRef(0);
  const lastOffRef = useRef(0);

  // ìµœì‹  ê°’ refs
  const accessTokenRef = useRef(accessToken);
  const recipeIdRef = useRef(recipeId);
  const onIntentRef = useRef(onIntent);
  const selectedSttModelRef = useRef(selectedSttModel);
  const onVoiceStartRef = useRef(onVoiceStart);
  const onVoiceEndRef = useRef(onVoiceEnd);
  const onVolumeRef = useRef(onVolume);
  const onKwsDetectionRef = useRef(onKwsDetection);
  const onKwsActivateRef = useRef(onKwsActivate);
  const onKwsDeactivateRef = useRef(onKwsDeactivate);

  useEffect(() => {
    accessTokenRef.current = accessToken;
  }, [accessToken]);
  useEffect(() => {
    recipeIdRef.current = recipeId;
  }, [recipeId]);
  useEffect(() => {
    onIntentRef.current = onIntent;
  }, [onIntent]);
  useEffect(() => {
    selectedSttModelRef.current = selectedSttModel;
  }, [selectedSttModel]);
  useEffect(() => {
    onVoiceStartRef.current = onVoiceStart;
  }, [onVoiceStart]);
  useEffect(() => {
    onVoiceEndRef.current = onVoiceEnd;
  }, [onVoiceEnd]);
  useEffect(() => {
    onVolumeRef.current = onVolume;
  }, [onVolume]);
  useEffect(() => {
    onKwsDetectionRef.current = onKwsDetection;
  }, [onKwsDetection]);
  useEffect(() => {
    onKwsActivateRef.current = onKwsActivate;
  }, [onKwsActivate]);
  useEffect(() => {
    onKwsDeactivateRef.current = onKwsDeactivate;
  }, [onKwsDeactivate]);

  // ------------------------
  // KWS Configuration
  // ------------------------
  const KWS_CONFIG = {
    TARGET_SR: 16000,
    WINDOW_SAMPLES: 16000, // 1s
    HOP_SAMPLES: 1600, // 100ms @16k
    threshold: 0.4,
    minSustainMs: 200,
    alpha: 0.3,
    timeoutMs: 3000, // 3ì´ˆ íƒ€ì„ì•„ì›ƒ
  };

  // ------------------------
  // KWS Functions
  // ------------------------
  const loadKwsModel = async () => {
    try {
      console.log('[KWS] ëª¨ë¸ ë¡œë”© ì¤‘...');
      const response = await fetch('/model_singlefile_v2.onnx');
      const arrayBuffer = await response.arrayBuffer();

      const options = {
        executionProviders: ['webgpu', 'wasm'],
      };

      const session = await ort.InferenceSession.create(arrayBuffer, options);
      kwsSessionRef.current = session;

      console.log('[KWS] ëª¨ë¸ ë¡œë“œ ì™„ë£Œ');
      console.log(`[KWS] ì…ë ¥: ${session.inputNames[0]}`);
      console.log(`[KWS] ì¶œë ¥: ${session.outputNames[0]}`);
    } catch (err: any) {
      console.error('[KWS] ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨:', err.message);
      setError(`KWS ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: ${err.message}`);
    }
  };

  const predictKws = async (audioChunk: Float32Array) => {
    try {
      const session = kwsSessionRef.current;
      if (!session) return null;

      const inputTensor = new ort.Tensor('float32', audioChunk, [1, audioChunk.length]);
      const feeds = { [session.inputNames[0]]: inputTensor };
      const results = await session.run(feeds);
      const logits = results[session.outputNames[0]].data as Float32Array;

      // 2-class softmax
      const m = Math.max(logits[0], logits[1]);
      const e0 = Math.exp(logits[0] - m);
      const e1 = Math.exp(logits[1] - m);
      return e1 / (e0 + e1);
    } catch (err: any) {
      console.error('[KWS] ì¶”ë¡  ì˜¤ë¥˜:', err.message);
      return null;
    }
  };

  const handleKwsDetection = (probToriya: number | null) => {
    if (probToriya == null) return;

    // EMA ìŠ¤ë¬´ë”©
    kwsEmaRef.current =
      kwsEmaRef.current == null
        ? probToriya
        : KWS_CONFIG.alpha * probToriya + (1 - KWS_CONFIG.alpha) * kwsEmaRef.current;

    const ema = kwsEmaRef.current;

    // ì½œë°±ìœ¼ë¡œ í™•ë¥  ì „ë‹¬
    onKwsDetectionRef.current?.(ema);

    if (ema >= KWS_CONFIG.threshold) {
      kwsSustainMsRef.current += (KWS_CONFIG.HOP_SAMPLES / KWS_CONFIG.TARGET_SR) * 1000;

      if (!kwsArmedRef.current && kwsSustainMsRef.current >= KWS_CONFIG.minSustainMs) {
        kwsArmedRef.current = true;
        onKwsActivation(probToriya, ema);
      }
    } else {
      kwsSustainMsRef.current = 0;
      kwsArmedRef.current = false;
    }
  };

  const onKwsActivation = (probToriya: number, ema: number) => {
    console.log(
      `[KWS] ğŸ¯ í† ë¦¬ì•¼ ê²€ì¶œ! í™•ë¥ : ${(probToriya * 100).toFixed(1)}%, EMA: ${(ema * 100).toFixed(1)}%`,
    );

    kwsActivatedRef.current = true;
    onKwsActivateRef.current?.();

    // 3ì´ˆ íƒ€ì„ì•„ì›ƒ ì„¤ì •
    if (kwsTimeoutRef.current) {
      clearTimeout(kwsTimeoutRef.current);
    }

    kwsTimeoutRef.current = setTimeout(() => {
      if (kwsActivatedRef.current && !speechActiveRef.current) {
        console.log('[KWS] 3ì´ˆ íƒ€ì„ì•„ì›ƒ - KWS ë¹„í™œì„±í™”');
        deactivateKws();
      }
    }, KWS_CONFIG.timeoutMs);
  };

  const deactivateKws = () => {
    kwsActivatedRef.current = false;
    kwsArmedRef.current = false;
    kwsEmaRef.current = null;
    kwsSustainMsRef.current = 0;
    kwsBufferRef.current = new Float32Array(0);

    if (kwsTimeoutRef.current) {
      clearTimeout(kwsTimeoutRef.current);
      kwsTimeoutRef.current = null;
    }

    onKwsDeactivateRef.current?.();
  };

  // ------------------------
  // WebSocket
  // ------------------------
  useEffect(() => {
    const openWS = () => {
      const url = new URL(STT_URL);
      url.searchParams.append('provider', selectedSttModelRef.current ?? 'VITO');
      const token = accessTokenRef.current;
      if (token) url.searchParams.append('token', token.replace(/^Bearer\s/i, ''));
      if (recipeIdRef.current) url.searchParams.append('recipe_id', recipeIdRef.current);

      const ws = new WebSocket(url.toString());
      ws.binaryType = 'arraybuffer';
      wsRef.current = ws;
      isWSReady.current = false;

      ws.onopen = () => {
        isWSReady.current = true;
      };
      ws.onmessage = ({ data }) => {
        try {
          const j = JSON.parse(data as string);
          console.log('[WS] message', j);
          if (j.status === 200 && j.data?.intent) {
            onIntentRef.current?.(j.data.intent); // í•„ìš” ì‹œ parseIntent
          }
        } catch {}
      };
      ws.onerror = e => {
        console.error('[WS] error', e);
        setError('WebSocket ì˜¤ë¥˜');
      };
      ws.onclose = () => {
        if (isMountedRef.current) {
          reconnectTimeoutRef.current = window.setTimeout(openWS, 500) as unknown as number;
        }
      };
    };

    openWS();
    return () => {
      if (reconnectTimeoutRef.current) {
        clearTimeout(reconnectTimeoutRef.current);
      }
      if (wsRef.current) {
        wsRef.current.onclose = null;
        wsRef.current.close(1000, 'page unmounted');
      }
    };
  }, []);

  // ------------------------
  // Audio + TEN VAD
  // ------------------------
  useEffect(() => {
    let destroyed = false;

    const start = async () => {
      try {
        // KWS ëª¨ë¸ ë¡œë“œ
        await loadKwsModel();

        console.log('[VAD] ëª¨ë“ˆ ë¡œë“œ ì‹œì‘...');
        const module = await VADModuleLoader.getInstance().loadModule();
        console.log('[VAD] ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ:', module);

        const hopSize = CHUNK_SIZE; // 10ms @ 16kHz
        const voiceThreshold = POS_TH;
        console.log(
          '[VAD] VAD ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì¤‘... hopSize:',
          hopSize,
          'threshold:',
          voiceThreshold,
        );
        const vad = new VADInstance(module, hopSize, voiceThreshold);
        vadInstanceRef.current = vad;
        console.log('[VAD] VAD ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ:', vad);

        // 3) ë§ˆì´í¬ ì˜¤í”ˆ
        console.log('[VAD] ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­ ì¤‘...');
        const stream = await navigator.mediaDevices.getUserMedia({
          audio: {
            channelCount: 1,
            sampleRate: SAMPLE_RATE,
            echoCancellation: true,
            noiseSuppression: true, // ì¥ì¹˜ë³„ A/B ê¶Œì¥
            autoGainControl: false,
          } as any,
          video: false,
        });
        if (destroyed) return;
        streamRef.current = stream;
        console.log('[VAD] ë§ˆì´í¬ ìŠ¤íŠ¸ë¦¼ íšë“ ì™„ë£Œ:', stream);

        const ctx = new AudioContext({ sampleRate: SAMPLE_RATE });
        audioCtxRef.current = ctx;
        console.log('[VAD] ì˜¤ë””ì˜¤ ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ. ìƒ˜í”Œë ˆì´íŠ¸:', ctx.sampleRate);

        const src = ctx.createMediaStreamSource(stream);

        // AudioWorklet í”„ë¡œì„¸ì„œ ë¡œë“œ ë° ìƒì„±
        await ctx.audioWorklet.addModule('/vad-processor.js');
        const vadWorklet = new AudioWorkletNode(ctx, 'vad-processor');
        processorRef.current = vadWorklet as any;
        console.log('[VAD] AudioWorklet í”„ë¡œì„¸ì„œ ìƒì„± ì™„ë£Œ');

        let processCount = 0;

        // AudioWorkletì—ì„œ ì˜¤ëŠ” ë©”ì‹œì§€ ì²˜ë¦¬
        vadWorklet.port.onmessage = async event => {
          const { type, chunks, rms } = event.data;

          if (type === 'audioData') {
            processCount++;
            if (processCount % 100 === 0) {
              console.log('[VAD] ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì‹± ì¤‘... count:', processCount);
            }

            const inst = vadInstanceRef.current;
            if (!inst) return;

            // ë³¼ë¥¨ ì½œë°±
            onVolumeRef.current?.(rms);

            const ws = wsRef.current;

            // ê° ì²­í¬ ì²˜ë¦¬
            for (const chunkArray of chunks) {
              const chunkF32 = new Float32Array(chunkArray);
              const i16 = f32ToI16(chunkF32);

              // 4) TEN VAD ì‹¤ì‹œê°„ í”„ë ˆì„ ì²˜ë¦¬
              const { probability } = await inst.processFrame(i16);

              if (processCount % 50 === 0) {
                console.log(
                  '[VAD] í™•ë¥ :',
                  probability.toFixed(3),
                  'í™œì„±:',
                  speechActiveRef.current,
                );
              }

              // 5) KWS ì²˜ë¦¬ (KWSê°€ ë¹„í™œì„±í™”ëœ ìƒíƒœì—ì„œë§Œ)
              if (!kwsActivatedRef.current) {
                // KWS ë²„í¼ì— ì²­í¬ ì¶”ê°€
                const mergedBuffer = new Float32Array(
                  kwsBufferRef.current.length + chunkF32.length,
                );
                mergedBuffer.set(kwsBufferRef.current);
                mergedBuffer.set(chunkF32, kwsBufferRef.current.length);
                kwsBufferRef.current = mergedBuffer;

                // 1ì´ˆ ìœˆë„ìš°ê°€ ì¤€ë¹„ë˜ë©´ KWS ì¶”ë¡  ì‹¤í–‰
                while (kwsBufferRef.current.length >= KWS_CONFIG.WINDOW_SAMPLES) {
                  const window = kwsBufferRef.current.slice(0, KWS_CONFIG.WINDOW_SAMPLES);
                  kwsBufferRef.current = kwsBufferRef.current.slice(KWS_CONFIG.HOP_SAMPLES);

                  // KWS ì¶”ë¡ 
                  const kwsProb = await predictKws(window);
                  handleKwsDetection(kwsProb);
                }
              }

              // Pre-buffer ê´€ë¦¬ (í•­ìƒ ìµœê·¼ ì²­í¬ë“¤ì„ ë³´ê´€)
              preBufferRef.current.push(chunkF32.slice()); // ë³µì‚¬ë³¸ ì €ì¥
              if (preBufferRef.current.length > PRE_BUFFER_CHUNKS) {
                preBufferRef.current.shift(); // ì˜¤ë˜ëœ ì²­í¬ ì œê±°
              }

              // 6) íˆìŠ¤í…Œë¦¬ì‹œìŠ¤ ê²Œì´íŒ… (KWSê°€ í™œì„±í™”ëœ ìƒíƒœì—ì„œë§Œ)
              const now = performance.now();
              let active = speechActiveRef.current;
              if (!active && kwsActivatedRef.current) {
                if (probability >= POS_TH) {
                  active = true;
                  speechActiveRef.current = true;
                  lastOnRef.current = now;
                  console.log(
                    '[VAD] ğŸ¤ ìŒì„± ì‹œì‘ ê°ì§€! í™•ë¥ :',
                    probability.toFixed(3),
                    'pre-buffer ì²­í¬:',
                    preBufferRef.current.length,
                  );

                  // Pre-bufferë¶€í„° ì „ì†¡ ì‹œì‘
                  if (ws && ws.readyState === WebSocket.OPEN && isWSReady.current) {
                    console.log('[VAD] Pre-buffer ì „ì†¡ ì‹œì‘');
                    for (const bufferedChunk of preBufferRef.current) {
                      // Pre-buffer ì²­í¬ë“¤ì„ 30ms ë‹¨ìœ„ë¡œ ì „ì†¡
                      let tx: Float32Array;
                      if (txLeftoverRef.current) {
                        const mergedTx = new Float32Array(
                          txLeftoverRef.current.length + bufferedChunk.length,
                        );
                        mergedTx.set(txLeftoverRef.current);
                        mergedTx.set(bufferedChunk, txLeftoverRef.current.length);
                        tx = mergedTx;
                        txLeftoverRef.current = null;
                      } else {
                        tx = bufferedChunk;
                      }

                      for (let off = 0; off + SEND_SIZE <= tx.length; off += SEND_SIZE) {
                        const slice = tx.subarray(off, off + SEND_SIZE);
                        const payload = f32ToI16(slice).buffer;
                        sendAudioData(ws, payload, false); // Pre-bufferëŠ” is_final=false
                      }
                      const txRest = tx.length % SEND_SIZE;
                      if (txRest) txLeftoverRef.current = tx.subarray(tx.length - txRest);
                    }
                    preBufferRef.current = []; // Pre-buffer ë¹„ìš°ê¸°
                  }

                  onVoiceStartRef.current?.();
                }
              } else {
                if (probability < NEG_TH && now - lastOnRef.current > ON_HOLD_MS) {
                  if (lastOffRef.current === 0) lastOffRef.current = now;
                  if (now - lastOffRef.current > OFF_HOLD_MS) {
                    active = false;
                    speechActiveRef.current = false;
                    lastOffRef.current = 0;
                    console.log('[VAD] ğŸ”‡ ìŒì„± ì¢…ë£Œ ê°ì§€! í™•ë¥ :', probability.toFixed(3));

                    // ìŒì„± ì¢…ë£Œ ì‹ í˜¸ ì „ì†¡ (is_final=true)
                    if (ws && ws.readyState === WebSocket.OPEN && isWSReady.current) {
                      // ë‚¨ì€ ë°ì´í„°ê°€ ìˆë‹¤ë©´ ë¨¼ì € ì „ì†¡
                      if (txLeftoverRef.current && txLeftoverRef.current.length > 0) {
                        const finalPayload = f32ToI16(txLeftoverRef.current).buffer;
                        sendAudioData(ws, finalPayload, true);
                        txLeftoverRef.current = null;
                      } else {
                        // ë¹ˆ ë°ì´í„°ë¡œë¼ë„ is_final ì‹ í˜¸ ì „ì†¡
                        sendAudioData(ws, new ArrayBuffer(0), true);
                      }
                      console.log('[VAD] ìŒì„± ì¢…ë£Œ ì‹ í˜¸ ì „ì†¡ (is_final=true)');
                    }

                    preBufferRef.current = []; // Pre-buffer ì´ˆê¸°í™”
                    onVoiceEndRef.current?.();

                    // KWS íƒ€ì„ì•„ì›ƒ ì¬ì„¤ì •
                    if (kwsActivatedRef.current) {
                      if (kwsTimeoutRef.current) {
                        clearTimeout(kwsTimeoutRef.current);
                      }
                      kwsTimeoutRef.current = setTimeout(() => {
                        console.log('[KWS] 3ì´ˆ íƒ€ì„ì•„ì›ƒ - KWS ë¹„í™œì„±í™”');
                        deactivateKws();
                      }, KWS_CONFIG.timeoutMs);
                    }
                  }
                } else {
                  lastOffRef.current = 0;
                }
              }

              // 7) ë°œí™” ì¤‘ì¼ ë•Œ í˜„ì¬ ì²­í¬ ì „ì†¡ (pre-bufferëŠ” ì´ë¯¸ ì „ì†¡ë¨, KWS í™œì„±í™” ìƒíƒœì—ì„œë§Œ)
              if (
                !ws ||
                ws.readyState !== WebSocket.OPEN ||
                !isWSReady.current ||
                !kwsActivatedRef.current
              )
                continue;

              if (active) {
                // ìŒì„± í™œì„± ìƒíƒœì—ì„œëŠ” í˜„ì¬ ì²­í¬ë¥¼ ë°”ë¡œ ì „ì†¡ (pre-buffer ì œì™¸)
                let tx: Float32Array;
                if (txLeftoverRef.current) {
                  const mergedTx = new Float32Array(txLeftoverRef.current.length + chunkF32.length);
                  mergedTx.set(txLeftoverRef.current);
                  mergedTx.set(chunkF32, txLeftoverRef.current.length);
                  tx = mergedTx;
                  txLeftoverRef.current = null;
                } else {
                  tx = chunkF32;
                }

                for (let off = 0; off + SEND_SIZE <= tx.length; off += SEND_SIZE) {
                  const slice = tx.subarray(off, off + SEND_SIZE);
                  const payload = f32ToI16(slice).buffer;
                  sendAudioData(ws, payload, false); // ì‹¤ì‹œê°„ ë°ì´í„°ëŠ” is_final=false
                }
                const txRest = tx.length % SEND_SIZE;
                if (txRest) txLeftoverRef.current = tx.subarray(tx.length - txRest);

                // í™œì„± ìƒíƒœì—ì„œëŠ” pre-bufferì— ì €ì¥í•˜ì§€ ì•ŠìŒ (ì‹¤ì‹œê°„ ì „ì†¡)
                preBufferRef.current = [];
              } else {
                txLeftoverRef.current = null; // ë¬´ìŒì€ ì „ì†¡ ë²„í¼ ì´ˆê¸°í™”
              }
            }
          }
        };

        src.connect(vadWorklet).connect(ctx.destination);
        console.log('[VAD] ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì—°ê²° ì™„ë£Œ');
      } catch (e: any) {
        console.error('[VAD] ì´ˆê¸°í™” ì‹¤íŒ¨:', e);
        setError(e?.message ?? 'ì˜¤ë””ì˜¤ ì´ˆê¸°í™” ì‹¤íŒ¨');
      }
    };

    start();

    return () => {
      destroyed = true;

      if (processorRef.current) {
        try {
          processorRef.current.disconnect();
          processorRef.current.port.onmessage = null;
        } catch {}
        processorRef.current = null;
      }
      if (audioCtxRef.current) {
        try {
          audioCtxRef.current.close();
        } catch {}
        audioCtxRef.current = null;
      }
      if (streamRef.current) {
        for (const t of streamRef.current.getTracks()) t.stop();
        streamRef.current = null;
      }
      if (vadInstanceRef.current) {
        try {
          vadInstanceRef.current.destroy();
        } catch {}
        vadInstanceRef.current = null;
      }
      // Pre-buffer ì´ˆê¸°í™”
      preBufferRef.current = [];
      txLeftoverRef.current = null;

      // KWS ì •ë¦¬
      if (kwsSessionRef.current) {
        try {
          // ONNX ì„¸ì…˜ì€ ìë™ìœ¼ë¡œ ì •ë¦¬ë¨
          kwsSessionRef.current = null;
        } catch {}
      }
      deactivateKws();
    };
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, []);

  useEffect(() => {
    return () => {
      isMountedRef.current = false;
    };
  }, []);

  return {
    error,
    isListening: speechActiveRef.current,
    isKwsActivated: kwsActivatedRef.current,
    stop: () => {
      if (audioCtxRef.current) audioCtxRef.current.suspend();
    },
  };
};

import { useEffect, useRef, useState } from 'react';

// TEN VAD exports (ë„ˆê°€ ì˜¬ë¦° index ê¸°ì¤€)
import type { TenVADInstance } from 'ten-vad-lib';
import { VADInstance, VADModuleLoader } from 'ten-vad-lib';

const STT_URL = 'wss://api.cheftories.com/api/v1/voice-command/ws';
const SAMPLE_RATE = 16000;
const CHUNK_SIZE = 160; // 10ms @ 16kHz
const BUFFER_CHUNKS = 3; // 30ms ë¬¶ìŒ
const SEND_SIZE = CHUNK_SIZE * BUFFER_CHUNKS;

// VAD ê²Œì´íŒ… íˆìŠ¤í…Œë¦¬ì‹œìŠ¤
const POS_TH = 0.6; // ì¼œì§ ì„ê³„
const NEG_TH = 0.5; // êº¼ì§ ì„ê³„
const ON_HOLD_MS = 150; // ì¼œì§ ìœ ì§€(ë°”ìš´ìŠ¤ ë°©ì§€)
const OFF_HOLD_MS = 250; // êº¼ì§ ì§€ì—°(í„´ ì¢…ë£Œ ì•ˆì •)

// Pre-buffer ì„¤ì • (ìŒì„± ì•ë¶€ë¶„ ë³´í˜¸)
const PRE_BUFFER_MS = 200; // 200ms í”„ë¦¬ë²„í¼
const PRE_BUFFER_CHUNKS = Math.ceil((PRE_BUFFER_MS * SAMPLE_RATE) / 1000 / CHUNK_SIZE); // ~12.5 ì²­í¬

const f32ToI16 = (f: Float32Array) => {
  const i = new Int16Array(f.length);
  for (let n = 0; n < f.length; n++) {
    const v = Math.max(-1, Math.min(1, f[n]));
    i[n] = (v * 0x7fff) | 0;
  }
  return i;
};

// ì„œë²„ í”„ë¡œí† ì½œì— ë§ê²Œ is_final í”Œë˜ê·¸ì™€ í•¨ê»˜ ì˜¤ë””ì˜¤ ì „ì†¡
const sendAudioData = (ws: WebSocket, audioData: ArrayBuffer, isFinal: boolean = false) => {
  const finalFlag = new Uint8Array([isFinal ? 1 : 0]);
  const audioBytes = new Uint8Array(audioData);
  const combined = new Uint8Array(finalFlag.length + audioBytes.length);
  combined.set(finalFlag, 0);
  combined.set(audioBytes, finalFlag.length);
  ws.send(combined.buffer);
};

interface Params {
  selectedSttModel?: string;
  accessToken?: string | null;
  recipeId?: string;
  onVoiceStart?: () => void;
  onVoiceEnd?: () => void;
  onIntent?: (i: any) => void; // BasicIntent ì“°ë©´ íƒ€ì… êµì²´
  onVolume?: (vol: number) => void;
}

export const useSimpleSpeech = ({
  selectedSttModel = 'CLOVA',
  accessToken,
  recipeId,
  onVoiceStart,
  onVoiceEnd,
  onIntent,
  onVolume,
}: Params) => {
  const [error, setError] = useState<string | null>(null);

  // WS
  const wsRef = useRef<WebSocket | null>(null);
  const isWSReady = useRef(false);
  const reconnectTimeoutRef = useRef<number | null>(null);

  // ì˜¤ë””ì˜¤
  const audioCtxRef = useRef<AudioContext | null>(null);
  const processorRef = useRef<AudioWorkletNode | null>(null);
  const streamRef = useRef<MediaStream | null>(null);

  // TEN VAD
  const vadInstanceRef = useRef<TenVADInstance | null>(null);

  // ìƒíƒœ
  const isMountedRef = useRef(true);
  const txLeftoverRef = useRef<Float32Array | null>(null);
  const preBufferRef = useRef<Float32Array[]>([]); // Pre-buffer for speech start protection

  // VAD ê²Œì´íŒ… ìƒíƒœ
  const speechActiveRef = useRef(false);
  const lastOnRef = useRef(0);
  const lastOffRef = useRef(0);

  // ìµœì‹  ê°’ refs
  const accessTokenRef = useRef(accessToken);
  const recipeIdRef = useRef(recipeId);
  const onIntentRef = useRef(onIntent);
  const selectedSttModelRef = useRef(selectedSttModel);

  useEffect(() => {
    accessTokenRef.current = accessToken;
  }, [accessToken]);
  useEffect(() => {
    recipeIdRef.current = recipeId;
  }, [recipeId]);
  useEffect(() => {
    onIntentRef.current = onIntent;
  }, [onIntent]);
  useEffect(() => {
    selectedSttModelRef.current = selectedSttModel;
  }, [selectedSttModel]);

  // ------------------------
  // WebSocket
  // ------------------------
  useEffect(() => {
    const openWS = () => {
      const url = new URL(STT_URL);
      url.searchParams.append('provider', selectedSttModelRef.current ?? 'VITO');
      const token = accessTokenRef.current;
      if (token) url.searchParams.append('token', token.replace(/^Bearer\s/i, ''));
      if (recipeIdRef.current) url.searchParams.append('recipe_id', recipeIdRef.current);

      const ws = new WebSocket(url.toString());
      ws.binaryType = 'arraybuffer';
      wsRef.current = ws;
      isWSReady.current = false;

      ws.onopen = () => {
        isWSReady.current = true;
      };
      ws.onmessage = ({ data }) => {
        try {
          const j = JSON.parse(data as string);
          console.log('[WS] message', j);
          if (j.status === 200 && j.data?.intent) {
            onIntentRef.current?.(j.data.intent); // í•„ìš” ì‹œ parseIntent
          }
        } catch {}
      };
      ws.onerror = e => {
        console.error('[WS] error', e);
        setError('WebSocket ì˜¤ë¥˜');
      };
      ws.onclose = () => {
        if (isMountedRef.current) {
          reconnectTimeoutRef.current = window.setTimeout(openWS, 500) as unknown as number;
        }
      };
    };

    openWS();
    return () => {
      if (reconnectTimeoutRef.current) {
        clearTimeout(reconnectTimeoutRef.current);
      }
      if (wsRef.current) {
        wsRef.current.onclose = null;
        wsRef.current.close(1000, 'page unmounted');
      }
    };
  }, []);

  // ------------------------
  // Audio + TEN VAD
  // ------------------------
  useEffect(() => {
    let destroyed = false;

    const start = async () => {
      try {
        console.log('[VAD] ëª¨ë“ˆ ë¡œë“œ ì‹œì‘...');
        const module = await VADModuleLoader.getInstance().loadModule();
        console.log('[VAD] ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ:', module);

        const hopSize = CHUNK_SIZE; // 10ms @ 16kHz
        const voiceThreshold = POS_TH;
        console.log(
          '[VAD] VAD ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì¤‘... hopSize:',
          hopSize,
          'threshold:',
          voiceThreshold,
        );
        const vad = new VADInstance(module, hopSize, voiceThreshold);
        vadInstanceRef.current = vad;
        console.log('[VAD] VAD ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ:', vad);

        // 3) ë§ˆì´í¬ ì˜¤í”ˆ
        console.log('[VAD] ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­ ì¤‘...');
        const stream = await navigator.mediaDevices.getUserMedia({
          audio: {
            channelCount: 1,
            sampleRate: SAMPLE_RATE,
            echoCancellation: true,
            noiseSuppression: true, // ì¥ì¹˜ë³„ A/B ê¶Œì¥
            autoGainControl: false,
          } as any,
          video: false,
        });
        if (destroyed) return;
        streamRef.current = stream;
        console.log('[VAD] ë§ˆì´í¬ ìŠ¤íŠ¸ë¦¼ íšë“ ì™„ë£Œ:', stream);

        const ctx = new AudioContext({ sampleRate: SAMPLE_RATE });
        audioCtxRef.current = ctx;
        console.log('[VAD] ì˜¤ë””ì˜¤ ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ. ìƒ˜í”Œë ˆì´íŠ¸:', ctx.sampleRate);

        const src = ctx.createMediaStreamSource(stream);

        // AudioWorklet í”„ë¡œì„¸ì„œ ë¡œë“œ ë° ìƒì„±
        await ctx.audioWorklet.addModule('/vad-processor.js');
        const vadWorklet = new AudioWorkletNode(ctx, 'vad-processor');
        processorRef.current = vadWorklet as any;
        console.log('[VAD] AudioWorklet í”„ë¡œì„¸ì„œ ìƒì„± ì™„ë£Œ');

        let processCount = 0;

        // AudioWorkletì—ì„œ ì˜¤ëŠ” ë©”ì‹œì§€ ì²˜ë¦¬
        vadWorklet.port.onmessage = async event => {
          const { type, chunks, rms } = event.data;

          if (type === 'audioData') {
            processCount++;
            if (processCount % 100 === 0) {
              console.log('[VAD] ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì‹± ì¤‘... count:', processCount);
            }

            const inst = vadInstanceRef.current;
            if (!inst) return;

            // ë³¼ë¥¨ ì½œë°±
            onVolume?.(rms);

            const ws = wsRef.current;

            // ê° ì²­í¬ ì²˜ë¦¬
            for (const chunkArray of chunks) {
              const chunkF32 = new Float32Array(chunkArray);
              const i16 = f32ToI16(chunkF32);

              // 4) TEN VAD ì‹¤ì‹œê°„ í”„ë ˆì„ ì²˜ë¦¬
              const { probability } = await inst.processFrame(i16);

              if (processCount % 50 === 0) {
                console.log(
                  '[VAD] í™•ë¥ :',
                  probability.toFixed(3),
                  'í™œì„±:',
                  speechActiveRef.current,
                );
              }

              // Pre-buffer ê´€ë¦¬ (í•­ìƒ ìµœê·¼ ì²­í¬ë“¤ì„ ë³´ê´€)
              preBufferRef.current.push(chunkF32.slice()); // ë³µì‚¬ë³¸ ì €ì¥
              if (preBufferRef.current.length > PRE_BUFFER_CHUNKS) {
                preBufferRef.current.shift(); // ì˜¤ë˜ëœ ì²­í¬ ì œê±°
              }

              // 5) íˆìŠ¤í…Œë¦¬ì‹œìŠ¤ ê²Œì´íŒ…
              const now = performance.now();
              let active = speechActiveRef.current;
              if (!active) {
                if (probability >= POS_TH) {
                  active = true;
                  speechActiveRef.current = true;
                  lastOnRef.current = now;
                  console.log(
                    '[VAD] ğŸ¤ ìŒì„± ì‹œì‘ ê°ì§€! í™•ë¥ :',
                    probability.toFixed(3),
                    'pre-buffer ì²­í¬:',
                    preBufferRef.current.length,
                  );

                  // Pre-bufferë¶€í„° ì „ì†¡ ì‹œì‘
                  if (ws && ws.readyState === WebSocket.OPEN && isWSReady.current) {
                    console.log('[VAD] Pre-buffer ì „ì†¡ ì‹œì‘');
                    for (const bufferedChunk of preBufferRef.current) {
                      // Pre-buffer ì²­í¬ë“¤ì„ 30ms ë‹¨ìœ„ë¡œ ì „ì†¡
                      let tx: Float32Array;
                      if (txLeftoverRef.current) {
                        const mergedTx = new Float32Array(
                          txLeftoverRef.current.length + bufferedChunk.length,
                        );
                        mergedTx.set(txLeftoverRef.current);
                        mergedTx.set(bufferedChunk, txLeftoverRef.current.length);
                        tx = mergedTx;
                        txLeftoverRef.current = null;
                      } else {
                        tx = bufferedChunk;
                      }

                      for (let off = 0; off + SEND_SIZE <= tx.length; off += SEND_SIZE) {
                        const slice = tx.subarray(off, off + SEND_SIZE);
                        const payload = f32ToI16(slice).buffer;
                        sendAudioData(ws, payload, false); // Pre-bufferëŠ” is_final=false
                      }
                      const txRest = tx.length % SEND_SIZE;
                      if (txRest) txLeftoverRef.current = tx.subarray(tx.length - txRest);
                    }
                    preBufferRef.current = []; // Pre-buffer ë¹„ìš°ê¸°
                  }

                  onVoiceStart?.();
                }
              } else {
                if (probability < NEG_TH && now - lastOnRef.current > ON_HOLD_MS) {
                  if (lastOffRef.current === 0) lastOffRef.current = now;
                  if (now - lastOffRef.current > OFF_HOLD_MS) {
                    active = false;
                    speechActiveRef.current = false;
                    lastOffRef.current = 0;
                    console.log('[VAD] ğŸ”‡ ìŒì„± ì¢…ë£Œ ê°ì§€! í™•ë¥ :', probability.toFixed(3));

                    // ìŒì„± ì¢…ë£Œ ì‹ í˜¸ ì „ì†¡ (is_final=true)
                    if (ws && ws.readyState === WebSocket.OPEN && isWSReady.current) {
                      // ë‚¨ì€ ë°ì´í„°ê°€ ìˆë‹¤ë©´ ë¨¼ì € ì „ì†¡
                      if (txLeftoverRef.current && txLeftoverRef.current.length > 0) {
                        const finalPayload = f32ToI16(txLeftoverRef.current).buffer;
                        sendAudioData(ws, finalPayload, true);
                        txLeftoverRef.current = null;
                      } else {
                        // ë¹ˆ ë°ì´í„°ë¡œë¼ë„ is_final ì‹ í˜¸ ì „ì†¡
                        sendAudioData(ws, new ArrayBuffer(0), true);
                      }
                      console.log('[VAD] ìŒì„± ì¢…ë£Œ ì‹ í˜¸ ì „ì†¡ (is_final=true)');
                    }

                    preBufferRef.current = []; // Pre-buffer ì´ˆê¸°í™”
                    onVoiceEnd?.();
                  }
                } else {
                  lastOffRef.current = 0;
                }
              }

              // 6) ë°œí™” ì¤‘ì¼ ë•Œ í˜„ì¬ ì²­í¬ ì „ì†¡ (pre-bufferëŠ” ì´ë¯¸ ì „ì†¡ë¨)
              if (!ws || ws.readyState !== WebSocket.OPEN || !isWSReady.current) continue;

              if (active) {
                // ìŒì„± í™œì„± ìƒíƒœì—ì„œëŠ” í˜„ì¬ ì²­í¬ë¥¼ ë°”ë¡œ ì „ì†¡ (pre-buffer ì œì™¸)
                let tx: Float32Array;
                if (txLeftoverRef.current) {
                  const mergedTx = new Float32Array(txLeftoverRef.current.length + chunkF32.length);
                  mergedTx.set(txLeftoverRef.current);
                  mergedTx.set(chunkF32, txLeftoverRef.current.length);
                  tx = mergedTx;
                  txLeftoverRef.current = null;
                } else {
                  tx = chunkF32;
                }

                for (let off = 0; off + SEND_SIZE <= tx.length; off += SEND_SIZE) {
                  const slice = tx.subarray(off, off + SEND_SIZE);
                  const payload = f32ToI16(slice).buffer;
                  sendAudioData(ws, payload, false); // ì‹¤ì‹œê°„ ë°ì´í„°ëŠ” is_final=false
                }
                const txRest = tx.length % SEND_SIZE;
                if (txRest) txLeftoverRef.current = tx.subarray(tx.length - txRest);

                // í™œì„± ìƒíƒœì—ì„œëŠ” pre-bufferì— ì €ì¥í•˜ì§€ ì•ŠìŒ (ì‹¤ì‹œê°„ ì „ì†¡)
                preBufferRef.current = [];
              } else {
                txLeftoverRef.current = null; // ë¬´ìŒì€ ì „ì†¡ ë²„í¼ ì´ˆê¸°í™”
              }
            }
          }
        };

        src.connect(vadWorklet).connect(ctx.destination);
        console.log('[VAD] ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì—°ê²° ì™„ë£Œ');
      } catch (e: any) {
        console.error('[VAD] ì´ˆê¸°í™” ì‹¤íŒ¨:', e);
        setError(e?.message ?? 'ì˜¤ë””ì˜¤ ì´ˆê¸°í™” ì‹¤íŒ¨');
      }
    };

    start();

    return () => {
      destroyed = true;

      if (processorRef.current) {
        try {
          processorRef.current.disconnect();
          processorRef.current.port.onmessage = null;
        } catch {}
        processorRef.current = null;
      }
      if (audioCtxRef.current) {
        try {
          audioCtxRef.current.close();
        } catch {}
        audioCtxRef.current = null;
      }
      if (streamRef.current) {
        for (const t of streamRef.current.getTracks()) t.stop();
        streamRef.current = null;
      }
      if (vadInstanceRef.current) {
        try {
          vadInstanceRef.current.destroy();
        } catch {}
        vadInstanceRef.current = null;
      }
      // Pre-buffer ì´ˆê¸°í™”
      preBufferRef.current = [];
      txLeftoverRef.current = null;
    };
  }, []);

  useEffect(() => {
    return () => {
      isMountedRef.current = false;
    };
  }, []);

  return {
    error,
    isListening: speechActiveRef.current,
    stop: () => {
      if (audioCtxRef.current) audioCtxRef.current.suspend();
    },
  };
};
